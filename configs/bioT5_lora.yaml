model:
  pretrained_model: QizhiPei/biot5-base
  out_dir: biot5_lora
  max_len: 384
  batch_size: 4
  lr: 5e-5
  n_epochs: 10
lora:
  r: 4
  alpha: 32
  dropout: 0.2
  enabled: True
data:
  subset: 100 # None for all data
  dataset: derm_qa
  # dataset: medXpert
  # dataset: no_robots

